import logging
from pathlib import Path
from typing import Optional

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from hive_zero_core.hive_mind import HiveMind
from hive_zero_core.training.config import ExperimentConfig, get_default_config
from hive_zero_core.training.data_loader import NetworkLogDataset
from hive_zero_core.training.rewards import CompositeReward


def train_hive_mind_adversarial(
    config: Optional[ExperimentConfig] = None,
    num_epochs: Optional[int] = None,
    lr: float = 1e-3,
    top_k: int = 5,
    observation_dim: int = 64,
    balance_weight: float = 0.1,
    blue_weight: float = 0.3,
    device: Optional[torch.device] = None,
):
    """
    Main adversarial co-evolutionary training loop for HIVE-ZERO.

    Trains the gating network, red-team experts, and blue-team detectors
    in an adversarial arms-race:

    * **Red objective** — minimise detection probability across the full
      blue-team stack (WAF + EDR + SIEM + IDS) while preserving payload
      diversity and stealth.
    * **Blue objective** — maximise detection probability on payloads
      generated by the red team this epoch.
    * **Threat Intel evolution** — every epoch, successful/failed payloads
      are recorded into the ThreatIntelDB, driving continuous adaptation.

    Args:
        config:          Experiment configuration. If None, uses default config.
        num_epochs:      Number of training epochs. Overrides config if provided.
        lr:              Peak learning rate for Adam.
        top_k:           Number of experts activated per forward pass.
        observation_dim: Observation embedding dimensionality.
        balance_weight:  Coefficient for the gating load-balance loss.
        blue_weight:     Coefficient for the blue-team detection loss.
        device:          Torch device; defaults to CUDA if available.
    """
    # Setup configuration
    if config is None:
        config = get_default_config()

    if num_epochs is not None:
        config.training.num_epochs = num_epochs
    logger = logging.getLogger("HiveTraining")
    logger.setLevel(getattr(logging, config.log_level))
    
    logger.info("=" * 80)
    logger.info("Starting HiveMind Adversarial Training")
    logger.info("=" * 80)
    logger.info(f"Experiment: {config.experiment_name}")
    logger.info(f"Epochs: {config.training.num_epochs}")
    logger.info(f"Learning Rate: {config.training.learning_rate}")
    logger.info(f"Batch Size: {config.data.batch_size}")

    # Set random seed for reproducibility
    torch.manual_seed(config.seed)

    # 1. Initialise HiveMind (includes ThreatIntelDB + all 19 experts)
    hive = HiveMind(observation_dim=observation_dim)
    if device is None:
        if config.device == "auto":
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            device = torch.device(config.device)
    hive.to(device)

    # 2. Separate optimisers for red and blue teams (adversarial)
    red_params = list(hive.gating_network.parameters())
    blue_params = []
    for expert in hive.experts:
        if expert.name in ("Cartographer", "DeepScope", "Chronos",
                           "Sentinel", "PayloadGen", "Mutator",
                           "Mimic", "Ghost", "Stego", "Cleaner",
                           "Tarpit", "FeedbackLoop", "Flashbang", "GlassHouse",
                           "PreAttackBooster"):
            red_params.extend(list(expert.parameters()))
        elif expert.name in ("WAF", "EDR", "SIEM", "IDS"):
            blue_params.extend(list(expert.parameters()))

    num_epochs_val = config.training.num_epochs
    red_optimizer = optim.Adam(red_params, lr=lr)
    blue_optimizer = optim.Adam(blue_params, lr=lr)
    red_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        red_optimizer, T_max=num_epochs_val
    )
    blue_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        blue_optimizer, T_max=num_epochs_val
    )

    # 3. Reward function
    reward_calc = CompositeReward()

    # 4. Training loop — alternating red/blue adversarial steps
    for epoch in range(num_epochs_val):

        # --- Environment step (mocked) ---
        mock_logs = [
            {'src_ip': '192.168.1.1', 'dst_ip': '10.0.0.5', 'port': 80, 'proto': 6},
            {'src_ip': '10.0.0.5', 'dst_ip': '8.8.8.8', 'port': 53, 'proto': 17},
        ]

        # ==============================================================
        # RED TEAM STEP — minimise blue-team detection
        # ==============================================================
        red_optimizer.zero_grad()
        results = hive.forward(mock_logs, top_k=top_k)

        red_loss = torch.tensor(0.0, device=device)

        # Adversarial evasion against Sentinel
        if "defense_score" in results:
            score = results["defense_score"].to(device)
            red_loss = red_loss + (-torch.mean(score))

        # Payload diversity
        if "optimized_payload" in results:
            payload = results["optimized_payload"].to(device)
            red_loss = red_loss + 0.01 * (-torch.std(payload, dim=-1).mean())

        # Information gain
        if "topology" in results:
            topology = results["topology"].to(device)
            current_entropy = float(torch.mean(torch.abs(topology)).item())
            r_info = reward_calc.calculate_info_gain_reward(0.8, current_entropy)
            red_loss = red_loss - torch.tensor(
                r_info, device=device, dtype=torch.float32
            )

        # Stealth reward
        if "traffic_shape" in results:
            traffic = results["traffic_shape"].to(device)
            baseline = torch.softmax(torch.randn_like(traffic), dim=-1)
            traffic_norm = torch.softmax(traffic, dim=-1)
            r_stealth = reward_calc.calculate_stealth_reward(traffic_norm, baseline)
            red_loss = red_loss + 0.1 * (-r_stealth)

        # Blue-team evasion: minimise P(Blocked) across all blue detectors
        for key in ("waf_verdict", "edr_verdict", "siem_verdict", "ids_verdict"):
            if key in results:
                logits = results[key].to(device)
                p_blocked = F.softmax(logits, dim=-1)[:, 0]
                red_loss = red_loss + blue_weight * p_blocked.mean()

        # Gating load-balance
        with torch.no_grad():
            data = hive.log_encoder.update(mock_logs)
            gs = hive.compute_global_state(data)
        weights = hive.gating_network(gs, top_k=top_k)
        red_loss = red_loss + balance_weight * hive.gating_network.load_balance_loss(weights)

        if red_loss.requires_grad:
            red_loss.backward()
            torch.nn.utils.clip_grad_norm_(red_params, max_norm=1.0)
            red_optimizer.step()

        red_scheduler.step()

        # ==============================================================
        # BLUE TEAM STEP — maximise detection of red-team payloads
        # ==============================================================
        blue_optimizer.zero_grad()

        # Re-run forward to get fresh blue-team logits
        results_blue = hive.forward(mock_logs, top_k=top_k)

        blue_loss = torch.tensor(0.0, device=device)
        for key in ("waf_verdict", "edr_verdict", "siem_verdict", "ids_verdict"):
            if key in results_blue:
                logits = results_blue[key].to(device)
                # Blue team wants to MAXIMISE P(Blocked) → minimise -log P(Blocked)
                log_probs = F.log_softmax(logits, dim=-1)
                blue_loss = blue_loss + (-log_probs[:, 0].mean())

        if blue_loss.requires_grad:
            blue_loss.backward()
            torch.nn.utils.clip_grad_norm_(blue_params, max_norm=1.0)
            blue_optimizer.step()

        blue_scheduler.step()

        # ==============================================================
        # THREAT INTEL EVOLUTION
        # ==============================================================
        hive.evolve_threat_intel(results)

        # --- Logging ---
        if epoch % max(1, num_epochs_val // 10) == 0:
            stats = hive.threat_intel.get_stats()
            logger.info(
                f"Epoch {epoch}/{num_epochs_val}  "
                f"red_loss={red_loss.item():.4f}  "
                f"blue_loss={blue_loss.item():.4f}  "
                f"gen={stats['generation']}  "
                f"evasion={stats['avg_evasion_rate']:.3f}  "
                f"lr={red_scheduler.get_last_lr()[0]:.6f}"
            )

        # Save checkpoint periodically
        if (epoch + 1) % config.training.save_frequency == 0:
            checkpoint_path = (
                config.training.checkpoint_dir / f"checkpoint_epoch_{epoch + 1}.pt"
            )
            save_checkpoint(hive, red_optimizer, epoch, red_loss.item(), checkpoint_path)
            logger.info(f"Checkpoint saved to {checkpoint_path}")

    logger.info("=" * 80)
    logger.info("Training complete!")
    logger.info("=" * 80)

    return hive


def save_checkpoint(
    model: HiveMind,
    optimizer: optim.Optimizer,
    epoch: int,
    loss: float,
    path: Path,
):
    """Save a training checkpoint."""
    path.parent.mkdir(parents=True, exist_ok=True)
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)


def load_checkpoint(
    model: HiveMind,
    optimizer: optim.Optimizer,
    path: Path,
) -> int:
    """
    Load a training checkpoint.

    Returns:
        The epoch number from the checkpoint.
    """
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch']



if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Quick test with 2 epochs
    from hive_zero_core.training.config import get_quick_test_config
    config = get_quick_test_config()
    train_hive_mind_adversarial(config=config)

