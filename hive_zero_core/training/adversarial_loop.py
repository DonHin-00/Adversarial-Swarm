import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import logging
from typing import Optional
from hive_zero_core.hive_mind import HiveMind
from hive_zero_core.training.rewards import CompositeReward


def train_hive_mind_adversarial(
    num_epochs: int = 10,
    lr: float = 1e-3,
    top_k: int = 5,
    observation_dim: int = 64,
    balance_weight: float = 0.1,
    blue_weight: float = 0.3,
    device: Optional[torch.device] = None,
):
    """
    Main adversarial co-evolutionary training loop for HIVE-ZERO.

    Trains the gating network, red-team experts, and blue-team detectors
    in an adversarial arms-race:

    * **Red objective** — minimise detection probability across the full
      blue-team stack (WAF + EDR + SIEM + IDS) while preserving payload
      diversity and stealth.
    * **Blue objective** — maximise detection probability on payloads
      generated by the red team this epoch.
    * **Threat Intel evolution** — every epoch, successful/failed payloads
      are recorded into the ThreatIntelDB, driving continuous adaptation.

    Args:
        num_epochs:      Number of training epochs.
        lr:              Peak learning rate for Adam.
        top_k:           Number of experts activated per forward pass.
        observation_dim: Observation embedding dimensionality.
        balance_weight:  Coefficient for the gating load-balance loss.
        blue_weight:     Coefficient for the blue-team detection loss.
        device:          Torch device; defaults to CUDA if available.
    """
    logger = logging.getLogger("HiveTraining")
    logger.setLevel(logging.INFO)

    # 1. Initialise HiveMind (includes ThreatIntelDB + all 19 experts)
    hive = HiveMind(observation_dim=observation_dim)
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    hive.to(device)

    # 2. Separate optimisers for red and blue teams (adversarial)
    red_params = list(hive.gating_network.parameters())
    blue_params = []
    for expert in hive.experts:
        if expert.name in ("Cartographer", "DeepScope", "Chronos",
                           "Mimic", "Ghost", "Stego", "Cleaner",
                           "Tarpit", "FeedbackLoop", "Flashbang", "GlassHouse",
                           "PreAttackBooster"):
            red_params.extend(list(expert.parameters()))
        elif expert.name in ("WAF", "EDR", "SIEM", "IDS"):
            blue_params.extend(list(expert.parameters()))

    red_optimizer = optim.Adam(red_params, lr=lr)
    blue_optimizer = optim.Adam(blue_params, lr=lr)
    red_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        red_optimizer, T_max=num_epochs
    )
    blue_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        blue_optimizer, T_max=num_epochs
    )

    # 3. Reward function
    reward_calc = CompositeReward()

    # 4. Training loop — alternating red/blue adversarial steps
    for epoch in range(num_epochs):

        # --- Environment step (mocked) ---
        mock_logs = [
            {'src_ip': '192.168.1.1', 'dst_ip': '10.0.0.5', 'port': 80, 'proto': 6},
            {'src_ip': '10.0.0.5', 'dst_ip': '8.8.8.8', 'port': 53, 'proto': 17},
        ]

        # ==============================================================
        # RED TEAM STEP — minimise blue-team detection
        # ==============================================================
        red_optimizer.zero_grad()
        results = hive.forward(mock_logs, top_k=top_k)

        red_loss = torch.tensor(0.0, device=device)

        # Adversarial evasion against Sentinel
        if "defense_score" in results:
            score = results["defense_score"].to(device)
            red_loss = red_loss + (-torch.mean(score))

        # Payload diversity
        if "optimized_payload" in results:
            payload = results["optimized_payload"].to(device)
            red_loss = red_loss + 0.01 * (-torch.std(payload, dim=-1).mean())

        # Information gain
        if "topology" in results:
            topology = results["topology"].to(device)
            current_entropy = float(torch.mean(torch.abs(topology)).item())
            r_info = reward_calc.calculate_info_gain_reward(0.8, current_entropy)
            red_loss = red_loss - torch.tensor(
                r_info, device=device, dtype=torch.float32
            )

        # Stealth reward
        if "traffic_shape" in results:
            traffic = results["traffic_shape"].to(device)
            baseline = torch.softmax(torch.randn_like(traffic), dim=-1)
            traffic_norm = torch.softmax(traffic, dim=-1)
            r_stealth = reward_calc.calculate_stealth_reward(traffic_norm, baseline)
            red_loss = red_loss + 0.1 * (-r_stealth)

        # Blue-team evasion: minimise P(Blocked) across all blue detectors
        for key in ("waf_verdict", "edr_verdict", "siem_verdict", "ids_verdict"):
            if key in results:
                logits = results[key].to(device)
                p_blocked = F.softmax(logits, dim=-1)[:, 0]
                red_loss = red_loss + blue_weight * p_blocked.mean()

        # Gating load-balance
        with torch.no_grad():
            data = hive.log_encoder.update(mock_logs)
            gs = (torch.mean(data.x, dim=0, keepdim=True)
                  if data.x.size(0) > 0
                  else torch.zeros(1, observation_dim, device=data.x.device))
        weights = hive.gating_network(gs)
        red_loss = red_loss + balance_weight * hive.gating_network.load_balance_loss(weights)

        if red_loss.requires_grad:
            red_loss.backward()
            torch.nn.utils.clip_grad_norm_(red_params, max_norm=1.0)
            red_optimizer.step()

        red_scheduler.step()

        # ==============================================================
        # BLUE TEAM STEP — maximise detection of red-team payloads
        # ==============================================================
        blue_optimizer.zero_grad()

        # Re-run forward to get fresh blue-team logits
        results_blue = hive.forward(mock_logs, top_k=top_k)

        blue_loss = torch.tensor(0.0, device=device)
        for key in ("waf_verdict", "edr_verdict", "siem_verdict", "ids_verdict"):
            if key in results_blue:
                logits = results_blue[key].to(device)
                # Blue team wants to MAXIMISE P(Blocked) → minimise -log P(Blocked)
                log_probs = F.log_softmax(logits, dim=-1)
                blue_loss = blue_loss + (-log_probs[:, 0].mean())

        if blue_loss.requires_grad:
            blue_loss.backward()
            torch.nn.utils.clip_grad_norm_(blue_params, max_norm=1.0)
            blue_optimizer.step()

        blue_scheduler.step()

        # ==============================================================
        # THREAT INTEL EVOLUTION
        # ==============================================================
        hive.evolve_threat_intel(results)

        # --- Logging ---
        if epoch % max(1, num_epochs // 10) == 0:
            stats = hive.threat_intel.get_stats()
            logger.info(
                f"Epoch {epoch}/{num_epochs}  "
                f"red_loss={red_loss.item():.4f}  "
                f"blue_loss={blue_loss.item():.4f}  "
                f"gen={stats['generation']}  "
                f"evasion={stats['avg_evasion_rate']:.3f}  "
                f"lr={red_scheduler.get_last_lr()[0]:.6f}"
            )

    logger.info("Training loop complete.")


if __name__ == "__main__":
    logging.basicConfig()
    train_hive_mind_adversarial(num_epochs=2)
